{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Assignment 6\n",
    "\n",
    "This assignment has weighting $3.5$.\n",
    "The first question about clustering has 35%, and the second question about tiny image classification has 65%.\n",
    "\n",
    "This is a challenging assignment, so I recommend you start early."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Clustering for handwritten digits\n",
    "\n",
    "Supervised learning requires labeled data, which can be expensive to acquire.\n",
    "For example, a dataset with $N$ samples for classification will require manual labeling $N$ times.\n",
    "\n",
    "One way to ameliorate this issue is to perform clustering of the raw data samples first, followed by manual inspection and labeling of only a few samples.\n",
    "Recall that clustering is a form of non-supervised learning, so it does not require any class labels.\n",
    "\n",
    "For example, say we are given a set of scanned hand-written digit images.\n",
    "We can cluster them into 10 groups first, manually inspect and label a few images in each cluster, and propagate the label towards the rest of all (unlabeled) samples in each cluster.\n",
    "\n",
    "The accuracy of such semi-automatic labeling depends on the accuracy of the clustering.\n",
    "If each cluster (0 to 9) corresponds exactly to hand-written digits 0-9, we are fine.\n",
    "Otherwise, we have some mis-labeled data.\n",
    "\n",
    "The goal of this question is to exercise clustering of the scikit-learn digits dataset which has labels, so that we can verify our clustering accuracy.\n",
    "The specifics are as follows.\n",
    "\n",
    "You will be judged by the test accuracy of your code, and quality of descriptions of your method.\n",
    "As a reference, a simple code I (Li-Yi) wrote can achieve about 78% accuracy. Try to beat it as much as you can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Training and test data split\n",
    "\n",
    "We will split the original dataset into training and test datasets\n",
    "* training for building our clusters\n",
    "* testing to see if the clusters can predict future data\n",
    "\n",
    "## Accuracy\n",
    "What is your clustering accuracy (comparing cluster labels with the ground truth labels), and what are the properties of mis-clustered samples?\n",
    "\n",
    "## Data preprocessing\n",
    "Would the original features (pixels) work well, or we need further processing like scaling/standardization or dimensionality-reduction, before clustering?\n",
    "\n",
    "## Models and hyper-parameters\n",
    "\n",
    "Let's focus on k-means clustering, as hierarchical and density-based clustering do not provide the predict() method under scikit-learn.\n",
    "\n",
    "What is the best test performance you can achieve with which hyper-parameters (for k-means, standard scalar, and dimensionality reduction)?\n",
    "\n",
    "### Hint\n",
    "We have learned Pipeline and GridSearchCV for cross validation and hyper-parameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last updated: 2016-12-14 \n",
      "\n",
      "CPython 3.5.2\n",
      "IPython 5.1.0\n",
      "\n",
      "numpy 1.11.1\n",
      "pandas 0.18.1\n",
      "matplotlib 1.5.3\n",
      "scipy 0.18.1\n",
      "sklearn 0.17.1\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a '' -u -d -v -p numpy,pandas,matplotlib,scipy,sklearn\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Added version check for recent scikit-learn 0.18 checks\n",
    "from distutils.version import LooseVersion as Version\n",
    "from sklearn import __version__ as sklearn_version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n",
      "(1797,)\n",
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "\n",
    "X = digits.data # data in pixels\n",
    "y = digits.target # digit labels\n",
    "type (X)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "print(np.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk4AAAGGCAYAAACNCg6xAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe826cd4358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "\n",
    "num_rows = 4\n",
    "num_cols = 5\n",
    "\n",
    "fig, ax = plt.subplots(nrows=num_rows, ncols=num_cols, sharex=True, sharey=True)\n",
    "ax = ax.flatten()\n",
    "for index in range(num_rows*num_cols):\n",
    "    img = digits.images[index]\n",
    "    label = digits.target[index]\n",
    "    ax[index].imshow(img, cmap='Greys', interpolation='nearest')\n",
    "    ax[index].set_title('digit ' + str(label))\n",
    "\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Data sets: training versus test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 1257, test: 540\n"
     ]
    }
   ],
   "source": [
    "if Version(sklearn_version) < '0.18':\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "else:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "num_training = y_train.shape[0]\n",
    "num_test = y_test.shape[0]\n",
    "print('training: ' + str(num_training) + ', test: ' + str(num_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[119 133 128 119 120 135 130 122 128 123]\n",
      "[59 49 49 64 61 47 51 57 46 57]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# check to see if the data are well distributed among digits\n",
    "for y in [y_train, y_test]:\n",
    "    print(np.bincount(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Answer\n",
    "These are the codes, explanation at the end. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We first write a scoring function for clustering so that we can use for GridSearchCV.\n",
    "Take a look at use_scorer under scikit learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "\n",
    "def clustering_accuracy_score(y_true, y_pred):\n",
    "    # replace this with your code; note that y_pred is just cluster id, not digit id\n",
    "    \n",
    "    concate = np.append(y_true, y_pred)\n",
    "    num_of_class = np.max(concate) + 1\n",
    "    #print(num_of_class)\n",
    "    \n",
    "    \n",
    "    size = y_true.size\n",
    "    \n",
    "    \n",
    "    correct = 0.0\n",
    "\n",
    "    confusion = np.zeros((num_of_class, num_of_class))\n",
    "    for i in range(0, size):\n",
    "        confusion[y_pred[i], y_true[i]] += 1\n",
    "        \n",
    "\n",
    "    for i in range(0, confusion.shape[0]):\n",
    "        \n",
    "        max_position = get_max_integer_in_ndarray(confusion)\n",
    "        correct += confusion[max_position[0],max_position[1]]\n",
    "        confusion = np.delete(confusion,max_position[0], axis = 0)\n",
    "        confusion = np.delete(confusion,max_position[1],axis = 1)\n",
    "    return correct / size \n",
    "\n",
    "# find the position of the max value in the matrix. \n",
    "def get_max_integer_in_ndarray(matrix):\n",
    "    pos_max_each_line = matrix.argmax(axis = 1)\n",
    "    max_values_each_line = np.zeros(pos_max_each_line.size)\n",
    "    for j in range(0, pos_max_each_line.size):\n",
    "            max_values_each_line[j]=matrix[j, pos_max_each_line[j]]\n",
    "    row_taken = max_values_each_line.argmax()\n",
    "    col_taken = pos_max_each_line[row_taken]\n",
    "    return (row_taken, col_taken)\n",
    "\n",
    "clustering_accuracy = make_scorer(clustering_accuracy_score) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 1.0 , should be 1\n",
      "accuracy 0.833333333333 , should be 0.8333333333333334\n",
      "accuracy 0.666666666667 , should be 0.6666666666666666\n",
      "accuracy 0.692307692308 , should be 0.692307692308\n"
     ]
    }
   ],
   "source": [
    "# toy case demonstrating the clustering accuracy\n",
    "# this is just a reference to illustrate what this score function is trying to achieve\n",
    "# feel free to design your own as long as you can justify\n",
    "\n",
    "# ground truth class label for samples\n",
    "toy_y_true = np.array([0, 0, 0, 1, 1, 2])\n",
    "\n",
    "# clustering id for samples\n",
    "toy_y_pred_true = np.array([1, 1, 1, 2, 2, 0])\n",
    "toy_y_pred_bad1 = np.array([0, 0, 1, 1, 1, 2])\n",
    "toy_y_pred_bad2 = np.array([2, 2, 1, 0, 0, 0])\n",
    "\n",
    "toy_accuracy = clustering_accuracy_score(toy_y_true, toy_y_pred_true)\n",
    "print('accuracy', toy_accuracy, ', should be 1')\n",
    "\n",
    "toy_accuracy = clustering_accuracy_score(toy_y_true, toy_y_pred_bad1)\n",
    "print('accuracy', toy_accuracy, ', should be', 5.0/6.0)\n",
    "\n",
    "toy_accuracy = clustering_accuracy_score(toy_y_true, toy_y_pred_bad2)\n",
    "print('accuracy', toy_accuracy, ', should be', 4.0/6.0)\n",
    "\n",
    "\n",
    "clus = np.array([1, 4, 4, 4, 4, 4, 3, 3, 2, 2, 3, 1, 1])\n",
    "clas = np.array([5, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 5, 2])\n",
    "print('accuracy', clustering_accuracy_score(clas, clus), ', should be 0.692307692308')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Build a pipeline with standard scaler, PCA, and clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# your code\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import Pipeline\n",
    "if Version(sklearn_version) < '0.18':\n",
    "    from sklearn.grid_search import GridSearchCV\n",
    "else:\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "pipe_est = Pipeline([('scl', StandardScaler()),\n",
    "                   ('pca', PCA()),\n",
    "                   ('kmeans', KMeans(n_clusters=10, init='k-means++'))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Use GridSearchCV to tune hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.677804295943\n",
      "{'pca__n_components': 46}\n"
     ]
    }
   ],
   "source": [
    "# your code\n",
    "pca_range = list(range(10, 64))\n",
    "param_grid = [{'pca__n_components': pca_range}]\n",
    "\n",
    "gs = GridSearchCV(estimator=pipe_est, \n",
    "                  param_grid=param_grid, \n",
    "                  scoring=clustering_accuracy, \n",
    "                  cv=10,\n",
    "                  n_jobs=24)#I was running on a 24-core server, tested on this setting, no bug found)\n",
    "\n",
    "gs = gs.fit(X_train, y_train)\n",
    "print (gs.best_score_)\n",
    "print(gs.best_params_)\n",
    "# below is Li-Yi's dummy code to build a random guess model\n",
    "import numpy as np\n",
    "class RandomGuesser:\n",
    "    def __init__(self, num_classes):\n",
    "        self.num_classes = num_classes\n",
    "    def predict(self, X):\n",
    "        y = np.random.randint(low = 0, high = self.num_classes, size = X.shape[0])\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.683\n"
     ]
    }
   ],
   "source": [
    "#best_model = RandomGuesser(10) # replace this with the best model you can build\n",
    "\n",
    "#y_cm = best_model.predict(X_test)\n",
    "y_cm = gs.predict(X_test)\n",
    "print('Test accuracy: %.3f' % clustering_accuracy_score(y_true=y_test, y_pred=y_cm))\n",
    "#print('Test accuracy: %.3f' % best_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Visualize mis-clustered samples, and provide your explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk4AAAGGCAYAAACNCg6xAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOk4iIiIikdRxEhEREYn0/wEgASsGm6ZbywAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe8265dbf60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# your code\n",
    "size = y_cm.size\n",
    "confusion = np.zeros((10, 10))\n",
    "for i in range(0, size):\n",
    "    confusion[y_cm[i], y_test[i]] += 1\n",
    "conf = confusion.copy()        \n",
    "\n",
    "\n",
    "## get the mappings between cluster and digit class \n",
    "#cluster[i]=x, the i-th cluster is the group of digit x. \n",
    "cluster_mapping = np.zeros(10)\n",
    "\n",
    "for i in range(0, 10):\n",
    "    #print(conf)\n",
    "    max_position = get_max_integer_in_ndarray(conf)\n",
    "    #print(max_position)\n",
    "    cluster_mapping[max_position[0]]=max_position[1]\n",
    "    conf[max_position[0],:]=0\n",
    "    conf[:,max_position[1]]=0\n",
    "#print(cluster_mapping)cluster_mapping\n",
    "\n",
    "wrongs = []\n",
    "for i in range (0, size):\n",
    "    if cluster_mapping[y_cm[i]] != y_test[i]:\n",
    "        wrongs.append(i)\n",
    "\n",
    "#print (1-len(wrongs) / 540.0)\n",
    "num_wrongs = len(wrongs)\n",
    "\n",
    "\n",
    "batch_size = 36\n",
    "num_cols = 9\n",
    "num_rows = 4\n",
    "\n",
    "def plot_batch(batch):\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=num_rows, ncols=num_cols, sharex=True, sharey=True)\n",
    "    ax = ax.flatten()\n",
    "    for i in range(0, batch_size):\n",
    "        index = wrongs[i+batch*batch_size]\n",
    "        img = X_test[index].reshape((8,8))\n",
    "        t = y_test[index]\n",
    "        p = cluster_mapping[y_cm[index]]\n",
    "        ax[i].imshow(img, cmap='Greys', interpolation='nearest')\n",
    "        ax[i].set_title('t' + str(t)+ ' p'+str(int(p)))\n",
    "\n",
    "    ax[0].set_xticks([])\n",
    "    ax[0].set_yticks([])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    \n",
    "num_batch_in_sample = 1 \n",
    "## print 36 samples of misclassfied, change this number if want more\n",
    "for i in range(0,num_batch_in_sample):\n",
    "    plot_batch(i) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# your explanation\n",
    "\n",
    "## Accuracy\n",
    "\n",
    "#### 1. How do I define which digit each cluster represents?\n",
    "\n",
    "Intuitively, voting is a good method. Because we know the label associate with each data, for each cluster, we can simply let each point vote for their own truth, count the vote and pick the digit with the most votes to represent that cluster. \n",
    "\n",
    "#### 2. How do I define accuracy?\n",
    "\n",
    "After the voting, we can *assume* that each cluster represents one digit. Then those different from that digit are counted as wrong. Accuracy can be defined by (total number of correct) / (data size). According to wikipedia, this shuold be called *purity*.\n",
    "\n",
    "#### 3. How many clusters I want to have?\n",
    "\n",
    "First in first, we won't have fewer than 10 clusters. Otherwise, it won't be helpful for our goal. \n",
    "\n",
    "If we have more than 10 clusters, which means different clusters can represent the same class, then the chance that different digit in the same cluster will decrease, and the accuracy (as we defined) will increase. However, more clusters means more manual work after getting the clustering result. One extreme case is to have one point per cluster, the accuracy is 100% but actually we are doing nothing. Moreover, if the cluster size is small, then the chance that the \"voting\" process diverge tends to higher. \n",
    "\n",
    "In this question, because we want mimic the process of using clustering to label digits, I assume we can have **10 and only 10 clusters** when we are doing clustering.\n",
    "\n",
    "### 4. How do I calcuate the accuracy score. \n",
    "\n",
    "Differnt from the definition of *purity*. I have a assumption that the number of cluster equals the number of classes (=10). Then after constructing the *confusion matrix* (according to wikipedia), rather than summing the max of each row directly, I do an iteration for assigning cluster to class.\n",
    "\n",
    "1. Pick the largest value in the *whole matrix*, say its index is $(i, j)$ assign that cluster-$i$ to that class-$j$. Count the correct points in cluster-$i$. \n",
    "2. Remove class-$j$ from the candidate list. (delete column-$j$ from the matrix)\n",
    "3. Go to next iteration until all clusters has been assigned. \n",
    "\n",
    "I did that because I must make sure that we have a 1-to-1 mapping between the 10 clustes and 10 classes. \n",
    "\n",
    "\n",
    "## Data preprocessing\n",
    "\n",
    "I used standard scaler and PCA for the preprocessing. And use grid search to find the N-components. However, because K-means have randomization, the best-value may change overtime. \n",
    "\n",
    "\n",
    "## Wrong cases\n",
    "Because the clustering is based on distance, rather than some features, it is hard to be very accurate. \n",
    "It is hard to determing some scribble handwritting. For example, 1 & 7 and 3 & 8, sometiems can be really close. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tiny image classification\n",
    "\n",
    "We will use the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html) for image object recognition.\n",
    "The dataset consists of 50000 training samples and 10000 test samples in 10 different classes (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck; see the link above for more information).\n",
    "The goal is to maximize the accuracy of your classifier on the test dataset after being optimized via the training dataset.\n",
    "\n",
    "You can use any learning models (supervised or unsupervised) or optimization methods (e.g. search methods for hyper-parameters).\n",
    "The only requirement is that your code can run inside an ipynb file, as usual.\n",
    "Please provide a description of your method, in addition to the code.\n",
    "Your answer will be evaluated not only on the test accuracy but also on the creativity of your methodology and the quality of your explanation/description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Sample code to get you started\n",
    "\n",
    "This is a difficult classification task.\n",
    "A sample code below, based on a simple fully connected neural network built via Keras, is provided below.\n",
    "The test accuracy is about 43%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last updated: 2016-12-14 \n",
      "\n",
      "CPython 3.5.2\n",
      "IPython 5.1.0\n",
      "\n",
      "numpy 1.11.1\n",
      "keras 1.1.1\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a '' -u -d -v -p numpy,keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "image shape:  (32, 32, 3)\n",
      "50000 training samples\n",
      "10000 test samples\n",
      "10 classes\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# load data set\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "print(X_train.shape)\n",
    "\n",
    "\n",
    "img_shape = X_train.shape[1:] # [num_rows, num_cols, num_channels]\n",
    "num_img_pixels = np.prod(img_shape)\n",
    "num_training_samples = X_train.shape[0]\n",
    "num_test_samples = X_test.shape[0]\n",
    "\n",
    "nb_classes = np.sum(np.unique(y_train).shape)\n",
    "\n",
    "print('image shape: ', img_shape)\n",
    "print(X_train.shape[0], 'training samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "print(nb_classes, 'classes')\n",
    "\n",
    "# data processing\n",
    "\n",
    "X_train = X_train.reshape(num_training_samples, num_img_pixels)\n",
    "X_test = X_test.reshape(num_test_samples, num_img_pixels)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# one hot encoding of labels\n",
    "y_train_ohe = np_utils.to_categorical(y_train)\n",
    "y_test_ohe = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# build a basic network\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(input_dim = num_img_pixels, \n",
    "                output_dim = 50, \n",
    "                init = 'uniform', \n",
    "                activation = 'tanh'))\n",
    "\n",
    "model.add(Dense(output_dim = 50, \n",
    "                init = 'uniform', \n",
    "                activation = 'tanh'))\n",
    "\n",
    "model.add(Dense(output_dim = nb_classes, \n",
    "                init = 'uniform', \n",
    "                activation = 'softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.001, decay=1e-7, momentum=.9)\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', \n",
    "              optimizer = sgd, \n",
    "              metrics = [\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# train\n",
    "\n",
    "_ = model.fit(X_train, y_train_ohe, \n",
    "              nb_epoch = 5, \n",
    "              batch_size = 10, \n",
    "              verbose = False, # turn this on to visualize progress \n",
    "              validation_split = 0.1 # 10% of training data for validation per epoch\n",
    "             )## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few predictions:  [6 1 9]\n",
      "Training accuracy: 0.43732\n",
      "Test accuracy: 0.4249\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "\n",
    "y_train_pred = model.predict_classes(X_train, verbose=False)\n",
    "print('First few predictions: ', y_train_pred[:3])\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "print('Training accuracy:', train_acc)\n",
    "\n",
    "y_test_pred = model.predict_classes(X_test, verbose=False)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "print('Test accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(50000, 3, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "# your code and experimental results\n",
    "from keras.constraints import maxnorm\n",
    "import theano\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Reshape\n",
    "#theano.config.openmp = True\n",
    "\n",
    "# re-load data set, don't change the shape (same code as given)\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "print (X_train.shape)\n",
    "\n",
    "\n",
    "img_shape = X_train.shape[1:] # [num_rows, num_cols, num_channels]\n",
    "num_img_pixels = np.prod(img_shape)\n",
    "num_training_samples = X_train.shape[0]\n",
    "num_test_samples = X_test.shape[0]\n",
    "\n",
    "nb_classes = np.sum(np.unique(y_train).shape)\n",
    "\n",
    "#print('image shape: ', img_shape)\n",
    "#print(X_train.shape[0], 'training samples')\n",
    "#print(X_test.shape[0], 'test samples')\n",
    "#print(nb_classes, 'classes')\n",
    "\n",
    "# data processing\n",
    "\n",
    "#X_train = X_train.reshape(num_training_samples, num_img_pixels)\n",
    "#X_test = X_test.reshape(num_test_samples, num_img_pixels)\n",
    "\n",
    "\n",
    "#transpose the data to the convolution input need \n",
    "#don't know whether there are smarter way \n",
    "#(like change the input parameter so that it can take other foramt of input) \n",
    "#just following the tutorial and use the same input format\n",
    "X_train=X_train.transpose(0, 3, 1, 2)\n",
    "X_test=X_test.transpose(0, 3, 1, 2)\n",
    "print(X_train.shape)\n",
    "\n",
    "\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# one hot encoding of labels\n",
    "y_train_ohe = np_utils.to_categorical(y_train)\n",
    "y_test_ohe = np_utils.to_categorical(y_test)\n",
    "\n",
    "\n",
    "model_new = Sequential()\n",
    "model_new.add(Convolution2D(32, 3, 3, input_shape=(3, 32, 32), dim_ordering='th', border_mode='same', activation='relu', W_constraint=maxnorm(3)))\n",
    "model_new.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model_new.add(Convolution2D(32, 3, 3, activation='relu',dim_ordering='th', border_mode='same', W_constraint=maxnorm(3)))\n",
    "model_new.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_new.add(Dropout(0.2))\n",
    "\n",
    "model_new.add(Flatten())\n",
    "model_new.add(Dense(3*16*16, activation='relu', W_constraint=maxnorm(3)))\n",
    "model_new.add(Reshape((3, 16, 16)))\n",
    "\n",
    "\n",
    "model_new.add(Convolution2D(32, 3, 3, activation='relu', dim_ordering='th', border_mode='same', W_constraint=maxnorm(3)))\n",
    "model_new.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_new.add(Dropout(0.2))    \n",
    "\n",
    "\n",
    "model_new.add(Flatten())\n",
    "    \n",
    "\n",
    "\n",
    "model_new.add(Dense(nb_classes, activation='softmax'))\n",
    "\n",
    "epochs = 25\n",
    "learning_rate = 0.01\n",
    "decay = learning_rate/epochs\n",
    "\n",
    "\n",
    "sgd = SGD(lr=learning_rate, momentum=0.9, decay=decay, nesterov=False)\n",
    "model_new.compile(loss = 'categorical_crossentropy', \n",
    "              optimizer = sgd, \n",
    "              metrics = [\"accuracy\"])\n",
    "\n",
    "#print(model_new.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "45000/45000 [==============================] - 128s - loss: 1.6712 - acc: 0.3865 - val_loss: 1.4181 - val_acc: 0.4828\n",
      "Epoch 2/10\n",
      "45000/45000 [==============================] - 128s - loss: 1.3399 - acc: 0.5190 - val_loss: 1.2091 - val_acc: 0.5596\n",
      "Epoch 3/10\n",
      "45000/45000 [==============================] - 128s - loss: 1.1697 - acc: 0.5809 - val_loss: 1.1501 - val_acc: 0.5960\n",
      "Epoch 4/10\n",
      "45000/45000 [==============================] - 128s - loss: 1.0349 - acc: 0.6302 - val_loss: 1.0965 - val_acc: 0.6182\n",
      "Epoch 5/10\n",
      "45000/45000 [==============================] - 128s - loss: 0.9211 - acc: 0.6706 - val_loss: 1.0792 - val_acc: 0.6210\n",
      "Epoch 6/10\n",
      "45000/45000 [==============================] - 128s - loss: 0.8201 - acc: 0.7086 - val_loss: 1.1072 - val_acc: 0.6282\n",
      "Epoch 7/10\n",
      "45000/45000 [==============================] - 128s - loss: 0.7196 - acc: 0.7444 - val_loss: 1.0793 - val_acc: 0.6406\n",
      "Epoch 8/10\n",
      "45000/45000 [==============================] - 128s - loss: 0.6339 - acc: 0.7729 - val_loss: 1.0881 - val_acc: 0.6472\n",
      "Epoch 9/10\n",
      "45000/45000 [==============================] - 128s - loss: 0.5536 - acc: 0.8031 - val_loss: 1.1016 - val_acc: 0.6472\n",
      "Epoch 10/10\n",
      "45000/45000 [==============================] - 128s - loss: 0.4819 - acc: 0.8316 - val_loss: 1.1641 - val_acc: 0.6448\n"
     ]
    }
   ],
   "source": [
    "_ = model_new.fit(X_train, y_train_ohe, \n",
    "              nb_epoch = 10, \n",
    "              batch_size = 10, \n",
    "              verbose = True, # turn this on to visualize progress \n",
    "              validation_split = 0.1 # 10% of training data for validation per epoch\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few predictions:  [6 9 9]\n",
      "Training accuracy: 0.907\n",
      "Test accuracy: 0.6423\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = model_new.predict_classes(X_train, verbose=False)\n",
    "print('First few predictions: ', y_train_pred[:3])\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "print('Training accuracy:', train_acc)\n",
    "\n",
    "y_test_pred = model_new.predict_classes(X_test, verbose=False)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (I got two more models below, I didn't change in the original code to make keep the previous output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(50000, 3, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "# your code and experimental results\n",
    "from keras.constraints import maxnorm\n",
    "import theano\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Reshape\n",
    "#theano.config.openmp = True\n",
    "\n",
    "# re-load data set, don't change the shape (same code as given)\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "print (X_train.shape)\n",
    "\n",
    "\n",
    "img_shape = X_train.shape[1:] # [num_rows, num_cols, num_channels]\n",
    "num_img_pixels = np.prod(img_shape)\n",
    "num_training_samples = X_train.shape[0]\n",
    "num_test_samples = X_test.shape[0]\n",
    "\n",
    "nb_classes = np.sum(np.unique(y_train).shape)\n",
    "\n",
    "#print('image shape: ', img_shape)\n",
    "#print(X_train.shape[0], 'training samples')\n",
    "#print(X_test.shape[0], 'test samples')\n",
    "#print(nb_classes, 'classes')\n",
    "\n",
    "# data processing\n",
    "\n",
    "#X_train = X_train.reshape(num_training_samples, num_img_pixels)\n",
    "#X_test = X_test.reshape(num_test_samples, num_img_pixels)\n",
    "\n",
    "\n",
    "#transpose the data to the convolution input need \n",
    "#don't know whether there are smarter way \n",
    "#(like change the input parameter so that it can take other foramt of input) \n",
    "#just following the tutorial and use the same input format\n",
    "X_train=X_train.transpose(0, 3, 1, 2)\n",
    "X_test=X_test.transpose(0, 3, 1, 2)\n",
    "print(X_train.shape)\n",
    "\n",
    "\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# one hot encoding of labels\n",
    "y_train_ohe = np_utils.to_categorical(y_train)\n",
    "y_test_ohe = np_utils.to_categorical(y_test)\n",
    "\n",
    "\n",
    "model_new_2 = Sequential()\n",
    "model_new_2.add(Convolution2D(32, 3, 3, input_shape=(3, 32, 32), dim_ordering='th', border_mode='same', activation='relu', W_constraint=maxnorm(3)))\n",
    "model_new_2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_new_2.add(Dropout(0.2))\n",
    "\n",
    "model_new_2.add(Convolution2D(32, 3, 3, activation='relu',dim_ordering='th', border_mode='same', W_constraint=maxnorm(3)))\n",
    "model_new_2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_new_2.add(Dropout(0.3))\n",
    "\n",
    "model_new_2.add(Flatten())\n",
    "model_new_2.add(Dense(3*16*16, activation='relu', W_constraint=maxnorm(3)))\n",
    "model_new_2.add(Reshape((3, 16, 16)))\n",
    "\n",
    "\n",
    "model_new_2.add(Convolution2D(32, 3, 3, activation='relu', dim_ordering='th', border_mode='same', W_constraint=maxnorm(3)))\n",
    "model_new_2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_new_2.add(Dropout(0.2))    \n",
    "\n",
    "\n",
    "model_new_2.add(Flatten())\n",
    "    \n",
    "\n",
    "\n",
    "model_new_2.add(Dense(nb_classes, activation='softmax'))\n",
    "\n",
    "epochs = 25\n",
    "learning_rate = 0.01\n",
    "decay = learning_rate/epochs\n",
    "\n",
    "\n",
    "sgd = SGD(lr=learning_rate, momentum=0.9, decay=decay, nesterov=False)\n",
    "model_new_2.compile(loss = 'categorical_crossentropy', \n",
    "              optimizer = sgd, \n",
    "              metrics = [\"accuracy\"])\n",
    "\n",
    "#print(model_new.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "45000/45000 [==============================] - 128s - loss: 1.7654 - acc: 0.3531 - val_loss: 1.3903 - val_acc: 0.4878\n",
      "Epoch 2/12\n",
      "45000/45000 [==============================] - 127s - loss: 1.3610 - acc: 0.5085 - val_loss: 1.2208 - val_acc: 0.5690\n",
      "Epoch 3/12\n",
      "45000/45000 [==============================] - 128s - loss: 1.1965 - acc: 0.5728 - val_loss: 1.1374 - val_acc: 0.5972\n",
      "Epoch 4/12\n",
      "45000/45000 [==============================] - 128s - loss: 1.0736 - acc: 0.6158 - val_loss: 1.0321 - val_acc: 0.6328\n",
      "Epoch 5/12\n",
      "45000/45000 [==============================] - 128s - loss: 0.9822 - acc: 0.6494 - val_loss: 0.9976 - val_acc: 0.6494\n",
      "Epoch 6/12\n",
      "45000/45000 [==============================] - 127s - loss: 0.8983 - acc: 0.6786 - val_loss: 0.9549 - val_acc: 0.6636\n",
      "Epoch 7/12\n",
      "45000/45000 [==============================] - 128s - loss: 0.8378 - acc: 0.7012 - val_loss: 0.9521 - val_acc: 0.6724\n",
      "Epoch 8/12\n",
      "45000/45000 [==============================] - 128s - loss: 0.7805 - acc: 0.7221 - val_loss: 0.9375 - val_acc: 0.6836\n",
      "Epoch 9/12\n",
      "45000/45000 [==============================] - 127s - loss: 0.7198 - acc: 0.7410 - val_loss: 0.9941 - val_acc: 0.6740\n",
      "Epoch 10/12\n",
      "45000/45000 [==============================] - 128s - loss: 0.6781 - acc: 0.7574 - val_loss: 0.9411 - val_acc: 0.6814\n",
      "Epoch 11/12\n",
      "45000/45000 [==============================] - 128s - loss: 0.6312 - acc: 0.7741 - val_loss: 0.9666 - val_acc: 0.6822\n",
      "Epoch 12/12\n",
      "45000/45000 [==============================] - 128s - loss: 0.5937 - acc: 0.7868 - val_loss: 0.9634 - val_acc: 0.6846\n"
     ]
    }
   ],
   "source": [
    "_ = model_new_2.fit(X_train, y_train_ohe, \n",
    "              nb_epoch = 12, \n",
    "              batch_size = 10, \n",
    "              verbose = True, # turn this on to visualize progress \n",
    "              validation_split = 0.1 # 10% of training data for validation per epoch\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (I got one more model below, that is the best I can get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few predictions:  [6 9 9]\n",
      "Training accuracy: 0.87436\n",
      "Test accuracy: 0.6652\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = model_new_2.predict_classes(X_train, verbose=False)\n",
    "print('First few predictions: ', y_train_pred[:3])\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "print('Training accuracy:', train_acc)\n",
    "\n",
    "y_test_pred = model_new_2.predict_classes(X_test, verbose=False)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(50000, 3, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "# your code and experimental results\n",
    "from keras.constraints import maxnorm\n",
    "import theano\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Reshape\n",
    "#theano.config.openmp = True\n",
    "\n",
    "# re-load data set, don't change the shape (same code as given)\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "print (X_train.shape)\n",
    "\n",
    "\n",
    "img_shape = X_train.shape[1:] # [num_rows, num_cols, num_channels]\n",
    "num_img_pixels = np.prod(img_shape)\n",
    "num_training_samples = X_train.shape[0]\n",
    "num_test_samples = X_test.shape[0]\n",
    "\n",
    "nb_classes = np.sum(np.unique(y_train).shape)\n",
    "\n",
    "#print('image shape: ', img_shape)\n",
    "#print(X_train.shape[0], 'training samples')\n",
    "#print(X_test.shape[0], 'test samples')\n",
    "#print(nb_classes, 'classes')\n",
    "\n",
    "# data processing\n",
    "\n",
    "#X_train = X_train.reshape(num_training_samples, num_img_pixels)\n",
    "#X_test = X_test.reshape(num_test_samples, num_img_pixels)\n",
    "\n",
    "\n",
    "#transpose the data to the convolution input need \n",
    "#don't know whether there are smarter way \n",
    "#(like change the input parameter so that it can take other foramt of input) \n",
    "#just following the tutorial and use the same input format\n",
    "X_train=X_train.transpose(0, 3, 1, 2)\n",
    "X_test=X_test.transpose(0, 3, 1, 2)\n",
    "print(X_train.shape)\n",
    "\n",
    "\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# one hot encoding of labels\n",
    "y_train_ohe = np_utils.to_categorical(y_train)\n",
    "y_test_ohe = np_utils.to_categorical(y_test)\n",
    "\n",
    "\n",
    "model_new_3 = Sequential()\n",
    "model_new_3.add(Convolution2D(32, 3, 3, input_shape=(3, 32, 32), dim_ordering='th', border_mode='same', activation='relu', W_constraint=maxnorm(3)))\n",
    "model_new_3.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_new_3.add(Dropout(0.2))\n",
    "\n",
    "model_new_3.add(Convolution2D(32, 3, 3, activation='relu',dim_ordering='th', border_mode='same', W_constraint=maxnorm(3)))\n",
    "model_new_3.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_new_3.add(Dropout(0.3))\n",
    "\n",
    "model_new_3.add(Flatten())\n",
    "model_new_3.add(Dense(3*16*16, activation='relu', W_constraint=maxnorm(3)))\n",
    "model_new_3.add(Dropout(0.2))\n",
    "model_new_3.add(Reshape((3, 16, 16)))\n",
    "\n",
    "\n",
    "model_new_3.add(Convolution2D(32, 3, 3, activation='relu', dim_ordering='th', border_mode='same', W_constraint=maxnorm(3)))\n",
    "model_new_3.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_new_3.add(Dropout(0.2))    \n",
    "\n",
    "\n",
    "model_new_3.add(Flatten())\n",
    "    \n",
    "\n",
    "\n",
    "model_new_3.add(Dense(nb_classes, activation='softmax'))\n",
    "\n",
    "epochs = 25\n",
    "learning_rate = 0.01\n",
    "decay = learning_rate/epochs\n",
    "\n",
    "\n",
    "sgd = SGD(lr=learning_rate, momentum=0.9, decay=decay, nesterov=False)\n",
    "model_new_3.compile(loss = 'categorical_crossentropy', \n",
    "              optimizer = sgd, \n",
    "              metrics = [\"accuracy\"])\n",
    "\n",
    "#print(model_new.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/15\n",
      "45000/45000 [==============================] - 128s - loss: 1.8262 - acc: 0.3297 - val_loss: 1.4226 - val_acc: 0.4890\n",
      "Epoch 2/15\n",
      "45000/45000 [==============================] - 128s - loss: 1.4126 - acc: 0.4875 - val_loss: 1.2731 - val_acc: 0.5632\n",
      "Epoch 3/15\n",
      "45000/45000 [==============================] - 128s - loss: 1.2508 - acc: 0.5478 - val_loss: 1.1477 - val_acc: 0.5952\n",
      "Epoch 4/15\n",
      "45000/45000 [==============================] - 128s - loss: 1.1489 - acc: 0.5894 - val_loss: 1.0468 - val_acc: 0.6264\n",
      "Epoch 5/15\n",
      "45000/45000 [==============================] - 128s - loss: 1.0669 - acc: 0.6163 - val_loss: 1.0573 - val_acc: 0.6278\n",
      "Epoch 6/15\n",
      "45000/45000 [==============================] - 128s - loss: 1.0057 - acc: 0.6411 - val_loss: 1.0033 - val_acc: 0.6498\n",
      "Epoch 7/15\n",
      "45000/45000 [==============================] - 128s - loss: 0.9563 - acc: 0.6581 - val_loss: 0.9670 - val_acc: 0.6606\n",
      "Epoch 8/15\n",
      "45000/45000 [==============================] - 129s - loss: 0.9078 - acc: 0.6758 - val_loss: 0.9670 - val_acc: 0.6616\n",
      "Epoch 9/15\n",
      "45000/45000 [==============================] - 128s - loss: 0.8693 - acc: 0.6900 - val_loss: 0.9471 - val_acc: 0.6738\n",
      "Epoch 10/15\n",
      "45000/45000 [==============================] - 128s - loss: 0.8295 - acc: 0.7046 - val_loss: 0.9061 - val_acc: 0.6814\n",
      "Epoch 11/15\n",
      "45000/45000 [==============================] - 128s - loss: 0.8000 - acc: 0.7141 - val_loss: 0.9075 - val_acc: 0.6858\n",
      "Epoch 12/15\n",
      "45000/45000 [==============================] - 129s - loss: 0.7727 - acc: 0.7247 - val_loss: 0.9010 - val_acc: 0.6900\n",
      "Epoch 13/15\n",
      "45000/45000 [==============================] - 129s - loss: 0.7445 - acc: 0.7335 - val_loss: 0.8832 - val_acc: 0.6970\n",
      "Epoch 14/15\n",
      "45000/45000 [==============================] - 129s - loss: 0.7170 - acc: 0.7431 - val_loss: 0.8947 - val_acc: 0.6900\n",
      "Epoch 15/15\n",
      "45000/45000 [==============================] - 129s - loss: 0.6977 - acc: 0.7525 - val_loss: 0.8965 - val_acc: 0.6930\n"
     ]
    }
   ],
   "source": [
    "_ = model_new_3.fit(X_train, y_train_ohe, \n",
    "              nb_epoch = 15, \n",
    "              batch_size = 10, \n",
    "              verbose = True, # turn this on to visualize progress \n",
    "              validation_split = 0.1 # 10% of training data for validation per epoch\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few predictions:  [6 9 9]\n",
      "Training accuracy: 0.8317\n",
      "Test accuracy: 0.6662\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = model_new_3.predict_classes(X_train, verbose=False)\n",
    "print('First few predictions: ', y_train_pred[:3])\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "print('Training accuracy:', train_acc)\n",
    "\n",
    "y_test_pred = model_new_3.predict_classes(X_test, verbose=False)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Your description\n",
    "with the following main parts:\n",
    "\n",
    "<dl>\n",
    "\n",
    "<dt>Introduction</dt>\n",
    "<dd>\n",
    "At a high-level, what are the approaches you considered and why?\n",
    "</dd>\n",
    "\n",
    "<dt>Method</dt>\n",
    "<dd>\n",
    "Describe your entire pipeline, such as data-preprocessing, model selection, and hyper-parameter optpimization.\n",
    "</dd>\n",
    "\n",
    "<dt>Results</dt>\n",
    "<dd>\n",
    "Describe the experimental process you took to arrive at the solution.\n",
    "For example:\n",
    "(1) compare your approach against other approach(es) you have tried, as\n",
    "well as the MLP baseline classifer.\n",
    "(2) compare against different settings of model parameters, e.g. regularization type/strength, number of hidden units or structure of a neural network, types of kernel in a SVM, etc. \n",
    "\n",
    "<dt>Conclusion</dt>\n",
    "<dd>\n",
    "Summarize what you have learned from the experiments and discuss the limitations and potential future improvements of your current method.\n",
    "</dd>\n",
    "\n",
    "<dt>References</dt>\n",
    "<dd>\n",
    "Cite any publically available code, blog posts, research papers, etc. you used or got ideas from.\n",
    "</dl>\n",
    "        First few predictions:  [6 9 9]\n",
    "        Training accuracy: 0.91506\n",
    "        Test accuracy: 0.6578"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer\n",
    "\n",
    "### Introduction\n",
    "\n",
    "*(At a high-level, what are the approaches you considered and why?)*\n",
    "\n",
    "\n",
    "Because it is an image recognition job, I first think of CNN. And then I learnt it through one Youtube video. (See \n",
    "\n",
    "\n",
    "1. The convolution layers (filtering) can extract features out. (What I have learnt in the *traditional* computer vision class). \n",
    "\n",
    "2. Pooling layers can help me to reduce the dimension with the output of convolution. \n",
    "\n",
    "3. The ReLu layers can help to normalize the output. \n",
    "\n",
    "4. The fully connected layer (dense First few predictions:   can help me to do the voting.\n",
    "\n",
    "5. Dropout layer can help me to prevent from overfitting. \n",
    "\n",
    "### Method\n",
    "\n",
    "I first use a Convolution layer to extract features, and then I use a pooling layer to aggregate the results. \n",
    "\n",
    "After that, then flatten the result and add a fully connected layers to do some classification. \n",
    "\n",
    "Then, I reconstruct it back and add one more convolution to extract more features. \n",
    "\n",
    "At last, I use the fully connected layer to generate the output. \n",
    "\n",
    "rease the **filter size** to 9. \n",
    "\n",
    "### Result\n",
    "\n",
    "At first use \n",
    "\n",
    "2 * (Convolution layer + MaxPool) + Fully Connected Layer + (Convolution + MaxPool) + Fully Connected. \n",
    "Each convolution layer has 64 channel, filter size = 25\n",
    "MaxPool = 2 x 2\n",
    "First Fully connected extract 64 class. \n",
    "Second will categorize the 10 classes. \n",
    "SGD: learning rate = 21\n",
    "Fit epoch = 5\n",
    "\n",
    "Because of the long time it take for training, I didn't use the GridSearch method to exhaust all the parameters, I just manually tuned the hyper-parameters. \n",
    "\n",
    "\n",
    "I got the result\n",
    "\n",
    "        First few predictions:  [6 9 9]\n",
    "        Training accuracy: 0.70032\n",
    "        Test accuracy: 0.6055\n",
    "\n",
    "I found that the vallidation loss is still decreasing (validation accuracy is still increasing), no converge, so I **increase the epoch from 5 to 10**. \n",
    "\n",
    "\n",
    "And then I got the result\n",
    "\n",
    "        First few predictions:  [6 9 9]\n",
    "        Training accuracy: 0.83826\n",
    "        Test accuracy: 0.6139\n",
    "\n",
    "\n",
    "I found that I am overfitting, so I add a **dropout layer**. According to the paper [Dropout: A simple way to prevent Neural Networks from Overfitting](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)\n",
    "    \n",
    "        \n",
    "        First few predictions: [6 9 9]\n",
    "        Training accuracy: 0.84944\n",
    "        Test accuracy: 0.6278in Keras)\n",
    "\n",
    "Still **overfitting**, so I decrese the complexity of the model, decrease the **channel** of the convolution for *several times*. I got the result \n",
    "\n",
    "        First few predictions:  [6 9 9]\n",
    "        Training accuracy: 0.88094\n",
    "        Test accuracy: 0.6447\n",
    "        \n",
    "        \n",
    "Still, and I decrease the **filter size** to 9. \n",
    "\n",
    "        First few predictions:  [6 9 9]\n",
    "        Training accuracy: 0.91506\n",
    "        Test accuracy: 0.6578\n",
    "        \n",
    "        \n",
    "        First few predictions:  [6 9 9]\n",
    "\n",
    "And then, add more dorpout. That is my final result.\n",
    "        First few predictions:  [6 9 9]\n",
    "        Training accuracy: 0.8317\n",
    "        Test accuracy: 0.6662\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "I have got a deeper understanding about the CNN after attempting to finish this homework. Specifically:\n",
    "\n",
    "1. The basic idea about what each layers are responsible for what tasks. \n",
    "\n",
    "2. Basic structuers of CNN and basic understandings of Keras. \n",
    "\n",
    "3. Don't use too complex model to deal with simple problems. That wil cause overfitting. \n",
    "\n",
    "4. Ways to dealing with Overfitting: **dropout** and **simplify models**.\n",
    "\n",
    "\n",
    "### Reference\n",
    "http://machinelearningmastery.com/object-recognition-convolutional-neural-networks-keras-deep-learning-library/\n",
    "https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721#.kazk6h3lm\n",
    "https://en.wikipedia.org/wiki/Convolutional_neural_network#Pooling_layerhttps://blog.rescale.com/neural-networks-using-keras-on-rescale/\n",
    "https://en.wikipedia.org/wiki/Convolutional_neural_network#Pooling_layer\n",
    "https://www.youtube.com/watch?v=FmpDIaiMIeA\n",
    "http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf\n",
    "https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Appendix\n",
    "Some raw data of the output. \n",
    "I didn't turn on the verbose at the begining. I forgot to copy down all the results while tuning the -\n",
    "\n",
    "\n",
    "        Train on 45000 samples, validate on 5000 samples\n",
    "        Epoch 1/10\n",
    "        45000/45000 [==============================] - 146s - loss: 1.9118 - acc: 0.2852 - val_loss: 1.5079 - val_acc: 0.4466\n",
    "        Epoch 2/10\n",
    "        45000/45000 [==============================] - 146s - loss: 1.4426 - acc: 0.4727 - val_loss: 1.3534 - val_acc: 0.5130\n",
    "        Epoch 3/10\n",
    "        45000/45000 [==============================] - 147s - loss: 1.2539 - acc: 0.5470 - val_loss: 1.1939 - val_acc: 0.5674\n",
    "        Epoch 4/10\n",
    "        45000/45000 [==============================] - 147s - loss: 1.1181 - acc: 0.6022 - val_loss: 1.1193 - val_acc: 0.6050\n",
    "        Epoch 5/10\n",
    "        45000/45000 [==============================] - 146s - loss: 1.0100 - acc: 0.6380 - val_loss: 1.0879 - val_acc: 0.6180\n",
    "        Epoch 6/10\n",
    "        45000/45000 [==============================] - 147s - loss: 0.9034 - acc: 0.6774 - val_loss: 1.0509 - val_acc: 0.6332\n",
    "        Epoch 7/10\n",
    "        45000/45000 [==============================] - 146s - loss: 0.8056 - acc: 0.7131 - val_loss: 1.0985 - val_acc: 0.6304\n",
    "        Epoch 8/10\n",
    "        45000/45000 [==============================] - 147s - loss: 0.7190 - acc: 0.7436 - val_loss: 1.1067 - val_acc: 0.6412\n",
    "        Epoch 9/10\n",
    "        45000/45000 [==============================] - 147s - loss: 0.6356 - acc: 0.7723 - val_loss: 1.1112 - val_acc: 0.6382\n",
    "        Epoch 10/10\n",
    "        45000/45000 [==============================] - 147s - loss: 0.5549 - acc: 0.8020 - val_loss: 1.1687 - val_acc: 0.6438\n",
    "\n",
    "\n",
    "        Epoch 1/10\n",
    "        45000/45000 [==============================] - 172s - loss: 1.7372 - acc: 0.3611 - val_loss: 1.3906 - val_acc: 0.4900\n",
    "        Epoch 2/10\n",
    "        45000/45000 [==============================] - 172s - loss: 1.3514 - acc: 0.5138 - val_loss: 1.2382 - val_acc: 0.5522\n",
    "        Epoch 3/10\n",
    "        45000/45000 [==============================] - 172s - loss: 1.1611 - acc: 0.5833 - val_loss: 1.1390 - val_acc: 0.5956\n",
    "        Epoch 4/10\n",
    "        45000/45000 [==============================] - 175s - loss: 1.0144 - acc: 0.6355 - val_loss: 1.0701 - val_acc: 0.6262\n",
    "        Epoch 5/10\n",
    "        45000/45000 [==============================] - 176s - loss: 0.8917 - acc: 0.6813 - val_loss: 1.0320 - val_acc: 0.6402\n",
    "        Epoch 6/10\n",
    "        45000/45000 [==============================] - 175s - loss: 0.7840 - acc: 0.7207 - val_loss: 1.0194 - val_acc: 0.6492\n",
    "        Epoch 7/10\n",
    "        45000/45000 [==============================] - 176s - loss: 0.6802 - acc: 0.7566 - val_loss: 1.0273 - val_acc: 0.6550\n",
    "        Epoch 8/10\n",
    "        45000/45000 [==============================] - 176s - loss: 0.5918 - acc: 0.7912 - val_loss: 1.0525 - val_acc: 0.6672\n",
    "        Epoch 9/10\n",
    "        45000/45000 [==============================] - 177s - loss: 0.5121 - acc: 0.8167 - val_loss: 1.0617 - val_acc: 0.6638\n",
    "        Epoch 10/10\n",
    "        45000/45000 [==============================] - 177s - loss: 0.4385 - acc: 0.8435 - val_loss: 1.1282 - val_acc: 0.6660"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "name": "_merged"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
